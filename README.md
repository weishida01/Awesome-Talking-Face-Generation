# Awesome Talking Face Generation [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Papers for Talking Head Generation, released codes collections.

# papers & codes 

## Survey

| title                                                        | accept      | paper                                                        |
| ------------------------------------------------------------ | ----------- | ------------------------------------------------------------ |
| Critical review of human face reenactment methods            | JIG 2022    | [paper](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=20220906) |
| Deep Learning for Visual Speech Analysis: A Survey           | Arxiv2022   | [paper](https://arxiv.org/abs/2205.10839)                    |
| Handbook of Digital Face Manipulation and Detection          | 2022        | [paper](https://library.oapen.org/bitstream/handle/20.500.12657/52835/978-3-030-87664-7.pdf?sequence=1) |
| Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis | Arxiv2021   | [paper](https://arxiv.org/abs/2109.02081)                    |
| The Creation and Detection of Deepfakes: A Survey            | Arxiv2020   | [paper](https://arxiv.org/abs/2004.11138)                    |
| DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection | Arxiv2020   | [paper](https://arxiv.org/abs/2001.00179)                    |
| What comprises a good talking-head video generation?: A Survey and Benchmark | Arxiv2020   | [paper](https://arxiv.org/abs/2005.03201)                    |
| Deep Audio-Visual Learning: A Survey                         | Arxiv2020   | [paper](http://arxiv.org/abs/2001.04758)                     |
| What comprises a good talking-head video generation?: A survey and benchmark | Arxiv2020   | [paper](https://arxiv.org/pdf/2005.03201.pdf)                |
| A Review on Face Reenactment Techniques                      | I4Tech 2020 | [paper](https://ieeexplore.ieee.org/document/9102668)        |

## 2023

| title | accept | paper | code | web/proj |keywords|
| --- | ---| --- | --- | --- | --- |
|DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis|Arxiv2023|[paper](https://arxiv.org/pdf/2301.03786.pdf)||[proj](https://sstzal.github.io/DiffTalk/)|Diffusion|
|Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation|Arxiv2023|[paper](https://mstypulkowski.github.io/diffusedheads/diffused_heads.pdf)||[proj](https://mstypulkowski.github.io/diffusedheads/)|Diffusion|
|Emotionally Enhanced Talking Face Generation|Arxiv2023|[paper](https://arxiv.org/pdf/2303.11548.pdf)|[code](https://github.com/sahilg06/EmoGen)|[webpage](https://midas.iiitd.edu.in/emo/)|emotion|
|Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos|Arxiv2023|[Paper](https://arxiv.org/abs/2305.03713)||[ProjectPage](https://research.nvidia.com/labs/nxp/avatar-fingerprinting/)||
|Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model|Arxiv2023|[Paper](https://arxiv.org/abs/2305.02594)||||
|StyleLipSync: Style-based Personalized Lip-sync Video Generation|Arxiv2023|[Paper](https://arxiv.org/abs/2305.00521)|[Code](https://github.com/TaekyungKi/StyleLipSync)|[ProjectPage](https://stylelipsync.github.io)||
|GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation|Arxiv2023|[Paper](https://arxiv.org/abs/2305.00787)||[ProjectPage](https://genefaceplusplus.github.io)||
|Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations|Arxiv2023|[Paper](https://arxiv.org/abs/2304.08945)||||
|That's What I Said: Fully-Controllable Talking Face Generation|Arxiv2023|[Paper](https://arxiv.org/abs/2304.03275)||[ProjectPage](https://mm.kaist.ac.kr/projects/FC-TFG/)||
|TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles|Arxiv2023|[Paper](https://arxiv.org/abs/2304.00334v1)||||
|DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder|Arxiv2023|[Paper](https://arxiv.org/abs/2303.17550)||[ProjectPage](https://daetalker.github.io/)||
|Style Transfer for 2D Talking Head Animation|Arxiv2023|[Paper](https://arxiv.org/abs/2303.09799)||||
|READ Avatars: Realistic Emotion-controllable Audio Driven Avatars|Arxiv2023|[Paper](https://arxiv.org/abs/2303.00744)||||
|On the Audio-visual Synchronization for Lip-to-Speech Synthesis|Arxiv2023|[Paper](https://arxiv.org/abs/2303.00502)||||
|EmoTalk: Speech-driven emotional disentanglement for 3D face animation|Arxiv2023|[Paper](https://arxiv.org/abs/2303.11089)||[ProjectPage](https://ziqiaopeng.github.io/emotalk/)|3D|
|FaceXHuBERT: Text-less Speech-driven E(X)pressive 3D Facial Animation Synthesis Using Self-Supervised Speech Representation Learning|Arxiv2023|[Paper](https://arxiv.org/abs/2303.05416)|[Code](https://github.com/galib360/FaceXHuBERT)|[ProjectPage](https://galib360.github.io/FaceXHuBERT/)|3D|
|Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertices Attention|Arxiv2023|[Paper](https://arxiv.org/abs/2302.12532)|||3D|
|Learning Audio-Driven Viseme Dynamics for 3D Face Animation|Arxiv2023|[Paper](https://arxiv.org/abs/2301.06059) [ProjectPage](https://linchaobao.github.io/viseme2023/)|||3D|
|CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior|Arxiv2023|[paper](https://arxiv.org/pdf/2301.02379.pdf)|[code](https://github.com/Doubiiu/CodeTalker)|[ProjectPage](https://doubiiu.github.io/projects/codetalker/)|3D|
|Expressive Speech-driven Facial Animation with controllable emotions|Arxiv2023|[Paper](https://arxiv.org/abs/2301.02008)|||3D|
|Speech Driven Video Editing via an Audio-Conditioned Diffusion Model|Arxiv2023|[paper](https://arxiv.org/pdf/2301.04474.pdf)|[code](https://github.com/DanBigioi/DiffusionVideoEditing)||Diffusion|
|Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis|CVPR2023|[paper](https://arxiv.org/abs/2211.14506)||[webpage](https://dorniwang.github.io/PD-FGC/)||
|One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field|CVPR2023|[paper](https://arxiv.org/abs/2304.05097)||[webpage](https://waytron.net/hidenerf/)||
|Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert|CVPR2023|[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.pdf)|[Code](https://github.com/Sxjdwang/TalkLip)|||
|LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook|CVPR2023|[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)||||
|High-Fidelity and Freely Controllable Talking Head Video Generation|CVPR2023|[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf)||[Project Page](https://yuegao.me/PECHead/)||
|High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning|CVPR2023|[paper](https://arxiv.org/abs/2305.02572)||||
|OTAvatar : One-shot Talking Face Avatar with Controllable Tri-plane Rendering|CVPR2023|[paper](https://arxiv.org/abs/2303.14662)|[code](https://github.com/theEricMa/OTAvatar)|||
|IP_LAP: Identity-Preserving Talking Face Generation with Landmark and Appearance Priors|CVPR2023||[code](https://github.com/Weizhi-Zhong/IP_LAP)|||
|SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation|CVPR2023|[paper](https://arxiv.org/abs/2211.12194)|[code](https://github.com/Winfredy/SadTalker)|[webpage](https://sadtalker.github.io/)||
|DPE: Disentanglement of Pose and Expression for General Video Portrait Editing|CVPR2023|[paper](https://arxiv.org/abs/2301.06281)|[code](https://github.com/Carlyx/DPE)|[webpage](https://carlyx.github.io/DPE/)||
|MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation|CVPR2023|[paper](https://arxiv.org/abs/2212.08062)|[code](https://github.com/Meta-Portrait/MetaPortrait)|[webpage](https://meta-portrait.github.io/)||
|GENEFACE: GENERALIZED AND HIGH-FIDELITY AUDIO-DRIVEN 3D TALKING FACE SYNTHESIS|ICLR2023|[paper](https://arxiv.org/pdf/2301.13430.pdf)|[code](https://github.com/yerfor/GeneFace)|[ProjectPage](https://geneface.github.io/)|NeRF|
|DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video|AAAI2023|[paper](https://fuxivirtualhuman.github.io/pdf/AAAI2023_FaceDubbing.pdf)|[code](https://github.com/MRzzm/DINet)|||
|StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles|AAAI2023|[paper](https://arxiv.org/pdf/2301.01081.pdf)|[code](https://github.com/FuxiVirtualHuman/styletalk)|||
|StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles|AAAI2023|[paper](https://arxiv.org/pdf/2301.01081.pdf)|[code](https://github.com/FuxiVirtualHuman/styletalk)|||
|Audio-Visual Face Reenactment|WACV2023|[paper](https://arxiv.org/pdf/2210.02755.pdf)|[code](https://github.com/mdv3101/AVFR-Gan)|[webpage](http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr)||
|Towards Generating Ultra-High Resolution Talking-Face Videos With Lip Synchronization|WACV2023|[paper](https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_Towards_Generating_Ultra-High_Resolution_Talking-Face_Videos_With_Lip_Synchronization_WACV_2023_paper.pdf)|-|||
|Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis|WACV2023|[paper](https://arxiv.org/pdf/2210.01559.pdf)|[Code](https://github.com/nihaomiao/WACV23_TSNet)|||
|DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions|ICASSP2023|[Paper](https://arxiv.org/abs/2303.07697)|[Code](https://github.com/deepbrainai-research/discohead)|[ProjectPage](https://deepbrainai-research.github.io/discohead/)||
|OPT: ONE-SHOT POSE-CONTROLLABLE TALKING HEAD GENERATION|ICASSP2023|[paper](https://arxiv.org/pdf/2302.08197.pdf)||||
|FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions|ICME 2023|[Paper](https://arxiv.org/abs/2303.17789)||||
|Compact Temporal Trajectory Representation for Talking Face Video Compression|TCSVT2023|[paper](https://ieeexplore.ieee.org/abstract/document/10109861/)||||
|HR-Net: a landmark based high realistic face reenactment network|TCSVT2023|[paper](https://ieeexplore.ieee.org/abstract/document/10103929/)||||
|A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation|MLSysWorkshop2023|[Paper](https://arxiv.org/abs/2304.00471v1)||||


## 2022

| title | accept | paper | code | web/proj |keywords|
| --- | ---| --- | --- | --- | --- |
|Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors|Arxiv2022|[paper](https://arxiv.org/pdf/2212.04248v1.pdf)||[proj](https://zxyin.github.io/TH-PAD/)||
|Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos|Arxiv2022|[paper](https://arxiv.org/pdf/2206.04523.pdf)||||
|DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation|Arxiv2022|[paper](https://arxiv.org/pdf/2203.07931.pdf)||||
|Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion|Arxiv2022|[paper](https://arxiv.org/pdf/2204.12756.pdf)||||
|AUTOLV: AUTOMATIC LECTURE VIDEO GENERATOR|Arxiv2022|[paper](https://arxiv.org/pdf/2209.08795v1.pdf)||||
|SPACE: Speech-driven Portrait Animation with Controllable Expression|Arxiv2022|||[ProjectPage](https://deepimagination.cc/SPACEx/)||
|Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement|Arxiv2022|[paper](https://arxiv.org/pdf/2209.01320v1.pdf)||||
|StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation|Arxiv2022|[paper](https://arxiv.org/abs/2208.10922)||||
|Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control|Arxiv2022|[Paper](https://arxiv.org/pdf/2208.02210.pdf)||||
|StableFace: Analyzing and Improving Motion Stability for Talking Face Generation|Arxiv2022|[Paper](https://arxiv.org/abs/2208.13717)||[ProjectPage](https://stable-face.github.io/)||
|StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN|Arxiv2022|[paper](https://arxiv.org/abs/2203.04036)|[code](https://github.com/FeiiYin/StyleHEAT)|[project page](https://feiiyin.github.io/StyleHEAT/)|stylegan|
|DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering|Arxiv2022|[paper](https://arxiv.org/abs/2201.00791)|||NeRF|
|Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions|Arxiv2022|[paper](https://arxiv.org/pdf/2204.06180.pdf)||||
|Imitator: Personalized Speech-driven 3D Facial Animation|Arxiv2022|[Paper](https://arxiv.org/abs/2301.00023)||[ProjectPage](https://balamuruganthambiraja.github.io/Imitator/)|3D|
|PV3D: A 3D Generative Model for Portrait Video Generation|Arxiv2022|[Paper](https://arxiv.org/abs/2212.06384)||[ProjectPage](https://showlab.github.io/pv3d/)|3D|
|Memories are One-to-Many Mapping Alleviators in Talking Face Generation|Arxiv2022|[Paper](https://arxiv.org/abs/2212.05005)||[ProjectPage](https://memoryface.github.io/)||
|AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars|NeurIPS2022|[paper](https://arxiv.org/pdf/2210.05825.pdf)||[Project](https://yuewuhkust.github.io/AniFaceGAN/)||
|VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild|SIGGRAPHASIA2022|[paper](https://arxiv.org/abs/2211.14758)|[code](https://github.com/vinthony/video-retalking)|||
|Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers|SIGGRAPHASIA2022|[Paper](https://arxiv.org/abs/2212.04970)||||
|EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model|SIGGRAPH2022|[paper](https://arxiv.org/pdf/2205.15278.pdf)|||emotion|
|FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment|PAMI2022|[paper](http://arxiv.org/abs/2202.12972)||||
|SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation|CVPR2022|[paper](https://arxiv.org/pdf/2211.12194v1.pdf)|[code](https://github.com/Winfredy/SadTalker)|||
|Expressive Talking Head Generation with Granular Audio-Visual Control|CVPR2022|[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf)|-|||
|Talking Face Generation With Multilingual TTS|CVPR2022|[paper](https://arxiv.org/abs/2205.06421)|[code](https://huggingface.co/spaces/CVPR/ml-talking-face)|-|-|
|Dual-Generator Face Reenactment|CVPR2022|[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Hsu_Dual-Generator_Face_Reenactment_CVPR_2022_paper.pdf)||||
|Depth-Aware Generative Adversarial Network for Talking Head Video Generation|CVPR2022|[paper](https://arxiv.org/abs/2203.06605)|[Code](https://github.com/harlanhong/CVPR2022-DaGAN)|[Project](https://harlanhong.github.io/publications/dagan.html)||
|HeadNeRF: A Real-time NeRF-based Parametric Head Model|CVPR2022|[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf)|[Code](https://github.com/CrisHY1995/headnerf)|[Project](https://hy1995.top/HeadNeRF-Project/)||
|I M Avatar: Implicit Morphable Head Avatars from Videos|CVPR2022|[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.pdf)|[Code](https://ait.ethz.ch/projects/2022/IMavatar/)|||
| Neural Emotion Director: Speech-preserving semantic control of facial expressions in “in-the-wild” videos | CVPR2022         | [Paper](https://arxiv.org/pdf/2112.00585.pdf)                | [Code](https://github.com/foivospar/NED)                     |||
|Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning|CVPR2022|[Paper](https://arxiv.org/pdf/2203.02573.pdf)|[Code](https://github.com/snap-research/MMVID)|[ProjectPage](https://snap-research.github.io/MMVID/)||
|Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis|ECCV2022|[paper](https://arxiv.org/abs/2207.11770)|[code](https://github.com/sstzal/DFRF)|[ProjectPage](https://sstzal.github.io/DFRF/)||
|Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation|ECCV2022|[paper](https://arxiv.org/pdf/2201.07786.pdf)|[code](https://github.com/alvinliu0/SSP-NeRF)|[ProjectPage](https://alvinliu0.github.io/projects/SSP-NeRF)|NeRF|
|StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pretrained StyleGAN|ECCV2022|[paper](https://arxiv.org/pdf/2203.04036.pdf)|[Code](https://github.com/FeiiYin/StyleHEAT/)|[Project](https://feiiyin.github.io/StyleHEAT/)||
|Realistic One-shot Mesh-based Head Avatars|ECCV2022|[paper](https://arxiv.org/pdf/2206.08343.pdf)||||
|Latent Image Animator: Learning to Animate Images via Latent Space Navigation|ICLR2022|[paper](https://openreview.net/pdf?id=7r6kDq0mK_)|[Code](https://github.com/wyhsirius/LIA)|[ProjectPage](https://wyhsirius.github.io/LIA-project/)||
|MegaPortraits: One-shot Megapixel Neural Head Avatars|ACMMM2022|[paper](https://arxiv.org/abs/2207.07621)||[Project](https://samsunglabs.github.io/MegaPortraits/)||
|Talking Head from Speech Audio using a Pre-trained Image Generator|ACMMM2022|[Paper](https://arxiv.org/pdf/2209.04252.pdf)||||
|Compressing Video Calls using Synthetic Talking Heads | BMVC2022 |[paper](https://arxiv.org/pdf/2210.03692.pdf)| | [webpage](https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression) |application |
|Finding Directions in GAN’s Latent Space for Neural Face Reenactment | BMVC2022 |[paper](https://arxiv.org/pdf/2202.00046.pdf)| [code](https://github.com/StelaBou/stylegan_directions_face_reenactment) | [Project](https://stelabou.github.io/stylegan-directions-reenactment/) | |
|Emotion-Controllable Generalized Talking Face Generation | IJCAI2022 |[paper](http://arxiv.org/abs/2205.01155)||||
| HifiHead: One-Shot High Fidelity Neural Head Synthesis with 3D Control] | IJCAI2022 |[paper](https://www.ijcai.org/proceedings/2022/0244.pdf)||||
|Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation|AAAI2022|[paper](https://www.aaai.org/AAAI22Papers/AAAI-6163.YangC.pdf) | -| - | -|
|SyncTalkFace: Talking Face Generation with Precise Lip-syncing via Audio-Lip Memory|AAAI2022| [paper(temp)](https://www.aaai.org/AAAI22Papers/AAAI-7528.ParkS.pdf) | - |   | - |
|One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning|AAAI2022| [paper](http://arxiv.org/abs/2112.02749) |  | [projectpage](https://github.com/FuxiVirtualHuman/AAAI22-one-shot-talking-face) |  |
|Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels|TMM2022| [paper](http://arxiv.org/abs/2201.05986) |  |  |  |
|Audio-driven Dubbing for User Generated Contents via Style-aware Semi-parametric Synthesis|TCSVT2022| [paper](https://ieeexplore.ieee.org/abstract/document/9903679/) |  |  |  |


## 2021
| title | accept | paper | code | web/proj |
| --- | ---| --- | --- | --- |
|AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person|Arxiv2021|[paper](https://arxiv.org/pdf/2108.04325v2.pdf)|||
|LandmarkGAN: Synthesizing Faces from Landmarks|Arxiv2021|[paper](http://arxiv.org/abs/2011.00269)|||
| 3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head | Arxiv2021        | [paper](https://arxiv.org/pdf/2104.12051.pdf)                |                                                              |                                                              |
|Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices|Arxiv2021| [paper](https://arxiv.org/pdf/2104.10299v1.pdf) | [code](https://github.com/choyingw/Voice2Mesh) ||
|Parallel and High-Fidelity Text-to-Lip Generation |Arxiv2021|[paper](https://arxiv.org/pdf/2107.06831v2.pdf)|| |
| Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation | SIGGRAPHASIA2021 | [paper](http://arxiv.org/abs/2109.10595)                     | [Code](https://github.com/YuanxunLu/LiveSpeechPortraits)     | [project page](https://github.com/YuanxunLu/LiveSpeechPortraits) |
|FaceFormer: Speech-Driven 3D Facial Animation with Transformers|CVPR2021|[paper](https://arxiv.org/abs/2112.05329)|[code](https://github.com/EvelynFan/FaceFormer)|[projectpage](https://github.com/EvelynFan/FaceFormer)|
|Audio-Driven Emotional Video Portraits|CVPR2021|[paper](https://arxiv.org/pdf/2104.07452.pdf)|[code](https://github.com/jixinya/EVP/)|[projectpage](https://github.com/jixinya/EVP)|
|LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization|CVPR2021|[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_paper.pdf)|-|-|
|Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation|CVPR2021|[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Pose-Controllable_Talking_Face_Generation_by_Implicitly_Modularized_Audio-Visual_Representation_CVPR_2021_paper.pdf)|[code](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)|[project page](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)|
|Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset|CVPR2021|[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf)|[code](https://github.com/MRzzm/HDTF)|[projectpage](https://github.com/MRzzm/HDTF)|
|Everything's Talkin': Pareidolia Face Reenactment|CVPR2021|[paper](http://arxiv.org/abs/2104.03061)|||
|NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction|CVPR2021|[paper](https://arxiv.org/pdf/2012.03065)|[Code](https://github.com/gafniguy/4D-Facial-Avatars)|[Project](https://gafniguy.github.io/4D-Facial-Avatars/)|
|One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing|CVPR2021|[paper](https://arxiv.org/abs/2011.15126)|[code](https://github.com/NVlabs/imaginaire)||
|Audio-Driven Emotional Video Portraits|CVPR2021|[Paper](https://jixinya.github.io/projects/evp/resources/evp.pdf)|[Code](https://github.com/jixinya/EVP/)||
|FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning|ICCV2021|[paper](https://arxiv.org/pdf/2108.07938v1.pdf)|[code](https://github.com/zhangchenxu528/FACIAL)|[projectpage](https://github.com/zhangchenxu528/FACIAL)|
|MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement|ICCV2021|[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf)|[code(coming soon)](https://github.com/facebookresearch/meshtalk)|[projectpage](https://github.com/facebookresearch/meshtalk)|
|AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis|ICCV2021|[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf)|[code](https://github.com/YudongGuo/AD-NeRF)|[projectpage](https://github.com/YudongGuo/AD-NeRF)|
|Sparse to Dense Motion Transfer for Face Image Animation|ICCV2021|[paper](https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Zhao_Sparse_to_Dense_Motion_Transfer_for_Face_Image_Animation_ICCVW_2021_paper.pdf)|||
|PIRenderer:PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering|ICCV2021|[paper](https://arxiv.org/pdf/2109.08379.pdf)|[Code](https://github.com/RenYurui/PIRender)||
|Learned Spatial Representations for Few-shot Talking-Head Synthesis|ICCV2021|[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Meshry_Learned_Spatial_Representations_for_Few-Shot_Talking-Head_Synthesis_ICCV_2021_paper.pdf)|||
|HeadGAN: One-shot Neural Head Synthesis and Editing|ICCV2021|[paper](https://arxiv.org/pdf/2012.08261.pdf)||[Project](https://michaildoukas.github.io/HeadGAN/)|
|S2D:Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis|ACMMM2021|[paper](https://arxiv.org/pdf/2111.00203v1.pdf)|[code](https://github.com/wuhaozhe/style_avatar) |[project page](https://github.com/wuhaozhe/style_avatar)|
| Towards Realistic Visual Dubbing with Heterogeneous Sources | ACMMM2021 | [paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475318) |                                                              ||
| One-shot Face Reenactment Using Appearance Adaptive Normalization | AAAI2021 | [paper](https://arxiv.org/pdf/2102.03984.pdf) |                                                              |                                                              |
| Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation | AAAI2021 | [paper](https://arxiv.org/pdf/2104.07995.pdf) | [code](https://github.com/FuxiVirtualHuman/Write-a-Speaker) |[projectpage](https://github.com/FuxiVirtualHuman/Write-a-Speaker)|
| Visual Speech Enhancement Without A Real Visual Stream       | WACV2021         | [paper](https://openaccess.thecvf.com/content/WACV2021/papers/Hegde_Visual_Speech_Enhancement_Without_a_Real_Visual_Stream_WACV_2021_paper.pdf) |  ||
| A unified framework for high fidelity face swap and expression reenactment | TCSVT2021        | [paper](https://ieeexplore.ieee.org/abstract/document/9517088/) |                                                              |  |
| Talking Head Generation with Audio and Speech Related Facial Action Units | BMVC2021         | [paper](http://arxiv.org/abs/2110.09951)                     |                                                              |  |
| LI-Net: Large-Pose Identity-Preserving Face Reenactment Network | ICME2021         | [paper](https://arxiv.org/pdf/2104.02850)                    |                                                              |  |
| 3D Talking Face with Personalized Pose Dynamics              | TVCG2021         | [paper](https://personal.utdallas.edu/~xxg061000/TVCG2021.pdf) |                                                              |  |
| Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion | IJCAI2021        | [paper](https://arxiv.org/pdf/2107.09293.pdf)                | [code](https://github.com/wangsuzhen/Audio2Head) | [projectpage](https://github.com/wangsuzhen/Audio2Head) |
| Speech2Talking-Face: Inferring and Driving a Face with Synchronized Audio-Visual Representation | IJCAI2021        | [paper](https://www.ijcai.org/proceedings/2021/0141.pdf)     |                                                              |                                                              |
|Text2Video: Text-driven Talking-head Video Synthesis with Phonetic Dictionary|ICASSP2021|[paper](https://arxiv.org/pdf/2104.14631v1.pdf)|[code](https://github.com/sibozhang/Text2Video)|[projectpage](https://github.com/sibozhang/Text2Video)|
|APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment| ICASSP2021 |[paper](https://arxiv.org/abs/2010.13017v1)|[code](https://github.com/zhangzjn/APB2FaceV2)|  |

## 2020
| title | accept | paper | code | web/proj | dataset |
| --- | ---| --- | --- | --- | --- |
| Everybody’s Talkin’: Let Me Talk as You Want                 | Arxiv2020        | [paper](http://arxiv.org/abs/2001.05201)                     |                                                              |                                                              |                                |
| Speech Driven Talking Face Generation from a Single Image and an Emotion Condition | Arxiv2020 | [paper](https://arxiv.org/pdf/2008.03592.pdf) | [code](https://github.com/eeskimez/emotalkingface) | [project page](https://github.com/eeskimez/emotalkingface) | CREMA-D |
| MakeItTalk: Speaker-Aware Talking Head Animation | Arxiv2020 | [paper](https://arxiv.org/pdf/2004.12992.pdf) | [code](https://github.com/adobe-research/MakeItTalk), [code](https://github.com/yzhou359/MakeItTalk) |                                                              | VoxCeleb2, VCTK |
| HeadGAN:Video-and-Audio-Driven Talking Head Synthesis | Arxiv2020 | [paper](https://arxiv.org/pdf/2012.08261v1.pdf) | - | | VoxCeleb2 |
| Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks | Arxiv2020 | [paper](https://arxiv.org/pdf/2002.08700.pdf) | - | | - |
| Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose | Arxiv2020 | [paper](https://arxiv.org/pdf/2002.10137.pdf) | [code](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose) | [projectpage](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose) | ImageNet,  FaceWarehouse,  LRW |
| SPEECH-DRIVEN FACIAL ANIMATION USING POLYNOMIAL FUSION OF FEATURES | Arxiv2020 | [paper](https://arxiv.org/pdf/1912.05833.pdf) | - | | LRW |
| MakeItTalk: Speaker-Aware Talking Head Animation             | SIGGRAPHASIA2020 | [paper](http://arxiv.org/abs/2004.12992)                     |[code](https://github.com/yzhou359/MakeItTalk)|[project page](https://github.com/yzhou359/MakeItTalk)||
|FReeNet: Multi-Identity Face Reenactment|CVPR2020|[paper](http://arxiv.org/abs/1905.11805)|[code](https://github.com/zhangzjn/FReeNet)|||
|Neural Head Reenactment with Latent Pose Descriptors|CVPR2020|[paper](http://arxiv.org/abs/2004.12000)|[code](https://github.com/shrubb/latent-pose-reenactment)|||
|Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis|CVPR2020|[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Prajwal_Learning_Individual_Speaking_Styles_for_Accurate_Lip_to_Speech_Synthesis_CVPR_2020_paper.pdf)||||
|Robust One Shot Audio to Video Generation|CVPRW2020|[Paper](https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Kumar_Robust_One_Shot_Audio_to_Video_Generation_CVPRW_2020_paper.html)||||
|Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment|CVPR2020|[paper](http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf)||||
| Talking-head Generation with Rhythmic Head Motion            | ECCV2020         | [paper](https://arxiv.org/pdf/2007.08547.pdf)                | [code](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion) | [projectpage](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion) | Crema, Grid, Voxceleb, Lrs3    |
| MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation | ECCV2020         | [paper](https://wywu.github.io/projects/MEAD/support/MEAD.pdf) | [code](https://github.com/uniBruce/Mead)                     | [Project](https://wywu.github.io/projects/MEAD/MEAD.html)    | VoxCeleb2, AffectNet           |
| Neural voice puppetry:Audio-driven facial reenactment        | ECCV2020         | [paper](https://arxiv.org/pdf/1912.05566.pdf)                | [Code](https://github.com/JustusThies/NeuralVoicePuppetry)   | [projectpage](https://github.com/miu200521358/NeuralVoicePuppetryMMD) | -                              |
| Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars | ECCV2020         | [paper](https://arxiv.org/pdf/2008.10174v1.pdf)              | [code](https://github.com/saic-violet/bilayer-model)         |                                                              | -                              |
| Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars | ECCV2020         | [paper](http://arxiv.org/abs/2008.10174)                     | [code](https://github.com/saic-violet/bilayer-model)         |                                                              |                                |
|Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose|AAAI2020|[paper](http://arxiv.org/abs/2003.12957)||||
|A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild | ACMMM2020 |[paper](https://arxiv.org/pdf/2008.10010.pdf) |[code](https://github.com/Rudrabha/Wav2Lip) | [project page](https://github.com/Rudrabha/Wav2Lip) | LRS2 |
|Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks | ACMMM2020 |[paper](http://arxiv.org/abs/2008.07783) | [Code](https://arxiv.org/abs/2008.07783) |  |  |
|Talking Face Generation with Expression-Tailored Generative Adversarial Network | ACMMM2020 |[Paper](https://dl.acm.org/doi/abs/10.1145/3394171.3413844) |  | | |
| Animating Face using Disentangled Audio Representations | WACV2020 | [paper](https://arxiv.org/pdf/1910.00726.pdf) | - | | |
| One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing | WACV2020         | [paper](http://arxiv.org/abs/2011.15126)                     |                                                              | [Project](https://nvlabs.github.io/face-vid2vid/)            |                                |
| FACEGAN: Facial Attribute Controllable rEenactment GAN       | WACV2020         | [paper](http://arxiv.org/abs/2011.04439)                     |                                                              |                                                              |                                |
|Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning|IJCAI2020|[paper](http://arxiv.org/abs/1812.06589)||  |  |
|APB2Face: Audio-guided face reenactment with auxiliary pose and blink signals|ICASSP2020|[paper](https://arxiv.org/abs/2004.14569v1)|[code](https://github.com/zhangzjn/APB2Face)|  |  |
| Realistic Speech-Driven Facial Animation with GANs | IJCV2020 | [paper](https://arxiv.org/pdf/1906.06337.pdf) |                                                              |  |  |
| Modality Dropout for Improved Performance-driven Talking Faces | ICMI2020         | [Paper](https://arxiv.org/abs/2005.13616)                    |                                                              |                                                              |                                |
|Multimodal inputs driven talking face generation with spatial–temporal dependency| TCSVT2020 |[paper](https://ieeexplore.ieee.org/abstract/document/8995571/)||||
| Multimodal Inputs Driven Talking Face Generation With Spatial-Temporal Dependency | TCSVT2020 | [paper](https://www.researchgate.net/profile/Jun_Yu42/publication/339224051_Multimodal_Inputs_Driven_Talking_Face_Generation_With_Spatial-Temporal_Dependency/links/5eae2c6a92851cb2676fa016/Multimodal-Inputs-Driven-Talking-Face-Generation-With-Spatial-Temporal-Dependency.pdf) |  |                                                              |  |

## 2019
| title | accept | paper | code | web/proj | dataset |
| --- | ---| --- | --- | --- | --- |
| FOMM:First order motion model for image animation            | NeurIPS2019 | [paper](http://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation.pdf) | [Code](https://github.com/AliaksandrSiarohin/first-order-model) |                                                              |               |
| Face Reconstruction from Voice using Generative Adversarial Networks | NeurIPS2019 | [paper](https://papers.nips.cc/paper/8768-face-reconstruction-from-voice-using-generative-adversarial-networks.pdf) |  | | |
|fs-vid2vid:Few-shot Video-to-Video Synthesis|NeurIPS2019|[paper](https://nvlabs.github.io/few-shot-vid2vid/main.pdf)|[Code](https://github.com/NVlabs/few-shot-vid2vid)|[Project](https://nvlabs.github.io/few-shot-vid2vid/)||
|Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss|CVPR2019|[paper](https://arxiv.org/pdf/1905.03820.pdf)|[code](https://github.com/lelechen63/ATVGnet)|[project page](https://github.com/lelechen63/ATVGnet)|VGG Face, LRW|
|Learning the Face Behind a Voice|CVPR2019|[paper](https://ieeexplore.ieee.org/document/8953196/)|[code](https://github.com/saiteja-talluri/Speech2Face)|||
|Hierarchical Cross-Modal Talking Face Generation with Dynamic Pixel-Wise Loss|CVPR2019|[paper](http://arxiv.org/abs/1905.03820)|[code](https://github.com/lelechen63/ATVGnet)|||
|Capture, Learning, and Synthesis of 3D Speaking Styles|CVPR2019|[Paper](http://openaccess.thecvf.com/content_CVPR_2019/html/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.html)||[projectpage](https://github.com/TimoBolkart/voca)||
|Monkey-Net:Animating Arbitrary Objects via Deep Motion Transfer|CVPR2019||[Code](https://github.com/AliaksandrSiarohin/monkey-net)|[Project](http://www.stulyakov.com/papers/monkey-net.html)]||
|Few-Shot Adversarial Learning of Realistic Neural Talking Head Models|ICCV2019|[paper](http://arxiv.org/abs/1905.08233)||||
|Make a Face: Towards Arbitrary High Fidelity Face Manipulation|ICCV2019|[paper](http://arxiv.org/abs/1908.07191)||||
|Talking Face Generation by Adversarially Disentangled Audio-Visual Representation|AAAI2019|[paper](http://arxiv.org/abs/1807.07860)|[Code](https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS)|[project page](https://github.com/Hangz-nju-cuhk/Talking-Face-Generation-DAVS)||
|FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis|AAAI2019|[paper](http://arxiv.org/abs/1911.09224)||||
|MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets|AAAI2019|[paper](http://arxiv.org/abs/1911.08139)||[Project](https://hyperconnect.github.io/MarioNETte/)||
|Towards Automatic Face-to-Face Translation|ACMMM2019|[paper](http://arxiv.org/abs/2003.00418)|[code](https://github.com/Rudrabha/LipGAN)|||
|Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks|ICASSP2019|[paper](http://arxiv.org/abs/1903.10195)|[code](https://github.com/miqueltubau/Wav2Pix)|||
|Talking Face Generation by Conditional Recurrent Adversarial Network|IJCAI2019|[paper]([https://papers.nips.cc/paper/8768-face-reconstruction-from-voice-using-generative-adversarial-networks.pdf](https://arxiv.org/pdf/1804.04786.pdf))|[code](https://github.com/susanqq/Talking_Face_Generation)|[project page](https://github.com/susanqq/Talking_Face_Generation)||


## datasets
- BIWI [project page](https://data.vision.ee.ethz.ch/cvl/datasets/b3dac2.en.html)
- CelebV [[project page](https://drive.google.com/file/d/1jQ6d76T5GQuvQH4dq8_Wq1T0cxvN0_xp/view)].
- CelebV-HQ [[project page](https://github.com/CelebV-HQ/CelebV-HQ)].
- CREMA-D [project page](https://github.com/CheyneyComputerScience/CREMA-D)
- CREMA-D 2014 [project page](https://github.com/CheyneyComputerScience/CREMA-D)
- DPCD [Link](https://github.com/Metaverse-AI-Lab-THU/Deep-Personalized-Character-Dataset-DPCD) [Paper](https://arxiv.org/abs/2304.11093)
- Faceforensics++ [[`Download link`](https://github.com/ondyari/FaceForensics)]
- GRID 2006 [project page](http://spandh.dcs.shef.ac.uk/avlombard/)
- HIT Bi-CAV 2005 
- HDTF 2020 [project page](https://github.com/MRzzm/HDTF)
- LRS2 [project page](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)
- LRS2-BBC 2018 [project page](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)
- LRS3-TED 2018 [project page](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html)
- LRW 2016 [project page](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)
- LRW-1000 2018 [project page](https://github.com/VIPL-Audio-Visual-Speech-Understanding/Lipreading-DenseNet3D)
- MSP-IMPROV 2016 [project page](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html)
- MELD 2018 [project page](https://affective-meld.github.io/)
- MEAD 2020 [project page](https://wywu.github.io/projects/MEAD/MEAD.html)
- MODALITY 2017 [project page](http://www.modality-corpus.org/)
- MMFace4D [project page](https://arxiv.org/abs/2303.09797)
- ObamaSet 2017
- RAVDESS 2018 [project page](https://sites.psychlabs.ryerson.ca/smartlab/resources/speech-song-database-ravdess/)
- SAVEE [project page](http://kahlan.eps.surrey.ac.uk/savee/Download.html)
- TalkingHead-1KH [[`Download link`](https://github.com/deepimagination/TalkingHead-1KH)].
- TCD-TIMIT 2015 [project page](https://sigmedia.tcd.ie/)
- VoxCeleb [project page](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)
- Voxceleb1 2017 [project page](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)
- Voxceleb2 2018 [project page](https://www.robots.ox.ac.uk/~vgg/data/voxceleb2/)

## metrics
| Metrics                                              | Paper                                                        | Link                                                         |
| ---------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| PSNR (peak signal-to-noise  ratio)                   | -                                                            |                                                              |
| SSIM (structural similarity  index measure)          | Image quality  assessment: from error visibility to structural similarity. |                                                              |
| CPBD(cumulative probability of  blur detection)      | A no-reference image  blur metric based on the cumulative probability of blur detection |                                                              |
| LPIPS (Learned Perceptual  Image Patch Similarity) - | The Unreasonable Effectiveness of Deep Features as  a Perceptual Metric | [paper](https://arxiv.org/pdf/1801.03924.pdf)                |
| NIQE (Natural Image Quality  Evaluator)              | Making a ‘Completely  Blind’ Image Quality Analyzer          | [paper](http://live.ece.utexas.edu/research/Quality/niqe_spl.pdf) |
| FID (Fréchet inception  distance)                    | GANs trained by a two  time-scale update rule converge to a local nash equilibrium |                                                              |
| LMD (landmark distance error)                        | Lip Movements Generation at a Glance                         |                                                              |
| LRA (lip-reading  accuracy)                          | Talking Face Generation by Conditional Recurrent  Adversarial Network | [paper](https://arxiv.org/pdf/1804.04786.pdf)                |
| WER(word error rate)                                 | Lipnet: end-to-end sentencelevel lipreading.                 |                                                              |
| LSE-D (Lip Sync Error -  Distance)                   | Out of time: automated lip sync in the wild                  |                                                              |
| LSE-C (Lip Sync Error -  Confidence)                 | Out of time: automated lip sync in the wild                  |                                                              |
| ACD(Average  content distance)                       | Facenet: a unified embedding for face recognition  and clustering. |                                                              |
| CSIM(cosine similarity)                              | Arcface: additive angular margin loss for deep face  recognition. |                                                              |
| EAR(eye aspect ratio)                                | Real-time eye blink  detection using facial landmarks. In: Computer Vision Winter Workshop |                                                              |
| ESD(emotion similarity  distance)                    | What comprises a good talking-head video  generation?: A Survey and Benchmark |                                                              |